{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "# !pip install tensorflow==2.0.0\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "from python_speech_features import ssc\n",
    "from scipy.fft import fft, ifft\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/speech/Data/Files/Thesis_data/coprime_noisy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"H5F.c\", line 509, in H5Fopen\n    unable to open file\n  File \"H5Fint.c\", line 1400, in H5F__open\n    unable to open file\n  File \"H5Fint.c\", line 1700, in H5F_open\n    unable to read superblock\n  File \"H5Fsuper.c\", line 623, in H5F__super_read\n    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'backup.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-807c77a276f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backup.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"can not be written\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"H5F.c\", line 509, in H5Fopen\n    unable to open file\n  File \"H5Fint.c\", line 1400, in H5F__open\n    unable to open file\n  File \"H5Fint.c\", line 1700, in H5F_open\n    unable to read superblock\n  File \"H5Fsuper.c\", line 623, in H5F__super_read\n    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'backup.h5'"
     ]
    }
   ],
   "source": [
    "backup=pd.HDFStore('backup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gccphat(signal):\n",
    "    gcc_mat=np.zeros((18,1))\n",
    "    d=0.2;\n",
    "    c=340;\n",
    "    max_lag=d/c*fs\n",
    "    a=0\n",
    "    j=[2,3,4,5,6,7,8]\n",
    "    for i in range(0,5):\n",
    "        for j in range (i+1,6):\n",
    "            N=(int(len(signal[:,i]))+int(len(signal[:,j])))-1\n",
    "            NFFT=int(np.power(2,np.ceil(np.log2(N))));\n",
    "            rng=range(int(NFFT/2+1-round(max_lag)),int(NFFT/2+1+round(max_lag)))\n",
    "#             print(rng)\n",
    "            center=round(max_lag)+1\n",
    "            Pxx=np.multiply(np.fft.fft(signal[:,i],NFFT),np.conj(np.fft.fft(signal[:,j],NFFT)))\n",
    "#             gcc=np.multiply(Pxx,(1/abs(Pxx)))\n",
    "            if all(abs(Pxx))!=0:\n",
    "                gcc=np.multiply(Pxx,(1/abs(Pxx)))\n",
    "            else:\n",
    "                gcc=Pxx\n",
    "            gcc_phat=np.fft.fftshift(np.fft.ifft(gcc))\n",
    "            phat_rng=gcc_phat[rng]\n",
    "            phat_reshape=np.reshape(phat_rng,(phat_rng.size,1))\n",
    "            gcc_mat=np.column_stack((gcc_mat,phat_reshape))\n",
    "    return (gcc_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=[]\n",
    "# foldername=20\n",
    "# file=np.array(os.listdir(path+'/20/20_train'))\n",
    "# data.append((file,foldername))\n",
    "# df=pd.DataFrame(file,columns=['X'])\n",
    "# df['Y']=foldername\n",
    "# df.to_excel(path+'/20/train_20.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=pd.DataFrame()\n",
    "# for i in range (0,25,5):\n",
    "#     k=str(i)\n",
    "#     file=np.array(os.listdir(path+'/'+k+'/'+k+'_val'))\n",
    "#     data.append((file,i))\n",
    "#     df=pd.DataFrame(file,columns=['X'])\n",
    "#     df['Y']=i\n",
    "#     df1=pd.concat([df1,df],ignore_index=True)\n",
    "# df1.to_excel(path+'/train1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see=tf.InteractiveSession()\n",
    "# Y=tf.one_hot(indices=[2],depth=5)\n",
    "# with tf.Session() as sess:\n",
    "#     print (sess.run(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data=pd.read_excel('train.xlsx')\n",
    "# plot_0=np.arange(0,960,40)\n",
    "# for i in plot_0:\n",
    "#     [fs,signal]=[fs,signal]=scipy.io.wavfile.read(path+'/train/'+data.loc[i]['X'])\n",
    "#     x=gccphat(signal)\n",
    "# #     print(x.shape,signal.shape)\n",
    "#     plt.matshow(abs(x),cmap='jet')\n",
    "# #     plt.plot(abs(x))\n",
    "#     plt.title(data.loc[i]['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [fs,signal]=scipy.io.wavfile.read(path+'/train/'+data.loc[100]['X'])\n",
    "# x=gccphat(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('train.xlsx')\n",
    "data_val=pd.read_excel('val.xlsx')\n",
    "data_test=pd.read_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "[[[0.         0.01212261 0.88982092 ... 0.01419678 0.05802027 0.00847979]\n",
      "  [0.         0.0185484  0.22265039 ... 0.07232488 0.02764731 0.08609726]\n",
      "  [0.         0.00481423 0.12694749 ... 0.02048023 0.01890041 0.02537095]\n",
      "  ...\n",
      "  [0.         0.06607636 0.01532873 ... 0.03034015 0.02889923 0.02555636]\n",
      "  [0.         0.04107699 0.01974527 ... 0.06763901 0.04481598 0.03976569]\n",
      "  [0.         0.03032074 0.00548434 ... 0.00665274 0.01082755 0.00452759]]\n",
      "\n",
      " [[0.         0.0114762  0.85324377 ... 0.02377169 0.00704683 0.0335283 ]\n",
      "  [0.         0.01292569 0.22366043 ... 0.02131183 0.01277648 0.04213623]\n",
      "  [0.         0.01306009 0.09485725 ... 0.0456304  0.04278146 0.07339251]\n",
      "  ...\n",
      "  [0.         0.06013417 0.00096446 ... 0.03923465 0.00790364 0.02614838]\n",
      "  [0.         0.04241624 0.01779081 ... 0.03402263 0.00867185 0.01075334]\n",
      "  [0.         0.0354077  0.01469167 ... 0.02910571 0.01743602 0.03439036]]\n",
      "\n",
      " [[0.         0.00771134 0.81384729 ... 0.02002042 0.00448338 0.02836824]\n",
      "  [0.         0.01398867 0.15657763 ... 0.01878882 0.07819285 0.03383037]\n",
      "  [0.         0.01535942 0.16073393 ... 0.05809647 0.03052909 0.06974926]\n",
      "  ...\n",
      "  [0.         0.06106866 0.02007684 ... 0.06330257 0.043446   0.05395441]\n",
      "  [0.         0.03847085 0.02570985 ... 0.01465056 0.02911459 0.00974436]\n",
      "  [0.         0.03878035 0.02179359 ... 0.0466217  0.03071339 0.04316734]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.02954642 0.30475026 ... 0.03149238 0.10910097 0.05320943]\n",
      "  [0.         0.02572753 0.66414376 ... 0.03801688 0.12280629 0.07779613]\n",
      "  [0.         0.02988543 0.12891946 ... 0.06448393 0.12977558 0.09738931]\n",
      "  ...\n",
      "  [0.         0.01251342 0.00210215 ... 0.02326882 0.03895824 0.02833853]\n",
      "  [0.         0.02864847 0.02882293 ... 0.04112994 0.01339697 0.06292542]\n",
      "  [0.         0.04030725 0.02435336 ... 0.00823038 0.05059775 0.03985779]]\n",
      "\n",
      " [[0.         0.02480015 0.36582347 ... 0.04156938 0.08829605 0.04900646]\n",
      "  [0.         0.03288462 0.81210967 ... 0.05231175 0.09516929 0.08431707]\n",
      "  [0.         0.03629442 0.21473276 ... 0.05362755 0.21112691 0.10264182]\n",
      "  ...\n",
      "  [0.         0.02740489 0.02063688 ... 0.02694825 0.0253818  0.05538389]\n",
      "  [0.         0.03377362 0.0198356  ... 0.03172966 0.03652381 0.03705701]\n",
      "  [0.         0.05483707 0.0148035  ... 0.01869944 0.03136545 0.04188443]]\n",
      "\n",
      " [[0.         0.02174225 0.37164594 ... 0.04418838 0.08776237 0.04793125]\n",
      "  [0.         0.03416605 0.81229634 ... 0.05455144 0.09056852 0.08142019]\n",
      "  [0.         0.03535696 0.20403672 ... 0.05260047 0.20186222 0.09477121]\n",
      "  ...\n",
      "  [0.         0.02952938 0.02652245 ... 0.02496338 0.02182002 0.05480441]\n",
      "  [0.         0.03306532 0.00972067 ... 0.02745523 0.04094286 0.03273205]\n",
      "  [0.         0.05616228 0.02122668 ... 0.01815935 0.02793064 0.043088  ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=[]\n",
    "print(len(data.loc[:]['X']))\n",
    "for i in range (0,len(data.loc[:]['X'])):\n",
    "    print(i)\n",
    "    [fs,signal]=scipy.io.wavfile.read(path+'/train/'+data.loc[i]['X'])\n",
    "    x=gccphat(signal)\n",
    "    train_data.append(abs(x))\n",
    "train_data=np.array(train_data)\n",
    "(np.array(train_data).shape)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 18, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val_data=[]\n",
    "print(len(data_val.loc[:]['X']))\n",
    "for i in range (0,len(data_val.loc[:]['X'])):\n",
    "    print(i)\n",
    "    [fs,signal]=scipy.io.wavfile.read(path+'/val/'+data_val.loc[i]['X'])\n",
    "    x=gccphat(signal)\n",
    "    val_data.append(abs(x))\n",
    "val_data=np.array(val_data)\n",
    "(np.array(val_data).shape)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 18, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data=[]\n",
    "print(len(data_test.loc[:]['X']))\n",
    "for i in range (0,len(data_test.loc[:]['X'])):\n",
    "    print(i)\n",
    "    [fs,signal]=scipy.io.wavfile.read(path+'/test/'+data_test.loc[i]['X'])\n",
    "    x=gccphat(signal)\n",
    "    test_data.append(abs(x))\n",
    "test_data=np.array(test_data)\n",
    "(np.array(test_data).shape)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t=[]\n",
    "# train_labels=[]\n",
    "# # see=tf.InteractiveSession()\n",
    "# for i in range (0,50):\n",
    "#     labels=data.loc[i]['Y']\n",
    "#     if labels == 5:\n",
    "#         labels = 1\n",
    "#     elif labels == 10:\n",
    "#         labels = 2\n",
    "# #     elif labels == 15:\n",
    "# #         labels = 3\n",
    "#     elif labels == 20:\n",
    "#         labels = 3\n",
    "# #     print(labels)\n",
    "#     Y=tf.one_hot(indices=[labels],depth=5)\n",
    "#     t=tf.Tensor.eval(Y)\n",
    "#     train_labels.append(t)\n",
    "# train_labels=np.array(train_labels)\n",
    "# # print(b[1])\n",
    "# #     t.append(Y)\n",
    "# #     with tf.Session() as sess:\n",
    "# #         print (sess.run(Y))\n",
    "# #     t=tf.stack([t,Y],0)\n",
    "# # print(t)\n",
    "# with tf.Session() as sess:\n",
    "#     print (sess.run(t))\n",
    "# (train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "v=np.max(train_data)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_data.npy',train_data)\n",
    "np.save('test_data.npy',test_data)\n",
    "np.save('val_data.npy',val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.load('train_data.npy')\n",
    "test_data=np.load('test_data.npy')\n",
    "val_data=np.load('val_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 1)\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "t=[]\n",
    "for i in range (0,len(data.loc[:]['Y'])):\n",
    "    labels=data.loc[i]['Y']\n",
    "    if labels == 5:\n",
    "        labels = 1\n",
    "    elif labels == 10:\n",
    "        labels = 2\n",
    "#     elif labels == 15:\n",
    "#         labels = 3\n",
    "    elif labels == 20:\n",
    "        labels = 4\n",
    "#     if t!=0:\n",
    "#         t=(np.divide(t,3)).tolist()\n",
    "    t.append(labels)\n",
    "t=np.array(t)\n",
    "# for i in range (0,len(t)):\n",
    "#     if t[i]!=0:\n",
    "#         t=np.divide(t[i],np.max(t))\n",
    "t=np.array(np.reshape(t,(len(data.loc[:]['Y']),1)),dtype=np.float32)\n",
    "# t=np.divide(t,np.max(t))\n",
    "print((t.shape))\n",
    "# encoded=to_categorical(t)\n",
    "# encoded[749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n"
     ]
    }
   ],
   "source": [
    "labels1=[]\n",
    "val_labels=[]\n",
    "for i in range (0,len(data_val.loc[:]['Y'])):\n",
    "    labels1=data_val.loc[i]['Y']\n",
    "#     if labels == 5:\n",
    "#         labels = 1\n",
    "#     elif labels == 10:\n",
    "#         labels = 2\n",
    "# #     elif labels == 15:\n",
    "# #         labels = 3\n",
    "#     elif labels == 20:\n",
    "#         labels = 4\n",
    "# #     if t!=0:\n",
    "#         t=(np.divide(t,3)).tolist()\n",
    "    val_labels.append(labels1)\n",
    "val_labels=np.array(val_labels)\n",
    "# for i in range (0,len(t)):\n",
    "#     if t[i]!=0:\n",
    "#         t=np.divide(t[i],np.max(t))\n",
    "val_labels=np.array(np.reshape(val_labels,(len(data_val.loc[:]['Y']),1)),dtype=np.float32)\n",
    "# t=np.divide(t,np.max(t))\n",
    "print((val_labels.shape))\n",
    "# encoded=to_categorical(t)\n",
    "# encoded[749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1)\n"
     ]
    }
   ],
   "source": [
    "labels2=[]\n",
    "test_labels=[]\n",
    "for i in range (0,len(data_test.loc[:]['Y'])):\n",
    "    labels2=data_test.loc[i]['Y']\n",
    "#     if labels == 5:\n",
    "#         labels = 1\n",
    "#     elif labels == 10:\n",
    "#         labels = 2\n",
    "# #     elif labels == 15:\n",
    "# #         labels = 3\n",
    "#     elif labels == 20:\n",
    "#         labels = 4\n",
    "# #     if t!=0:\n",
    "#         t=(np.divide(t,3)).tolist()\n",
    "    test_labels.append(labels2)\n",
    "test_labels=np.array(test_labels)\n",
    "# for i in range (0,len(t)):\n",
    "#     if t[i]!=0:\n",
    "#         t=np.divide(t[i],np.max(t))\n",
    "test_labels=np.array(np.reshape(test_labels,(len(data_test.loc[:]['Y']),1)),dtype=np.float32)\n",
    "# t=np.divide(t,np.max(t))\n",
    "print((test_labels.shape))\n",
    "# encoded=to_categorical(t)\n",
    "# encoded[749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((18, 16), (1,)), types: (tf.float64, tf.float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data,t))\n",
    "(train_dataset)\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((val_data,val_labels))\n",
    "(val_dataset)\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices((test_data,test_labels))\n",
    "(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = train_dataset.make_one_shot_iterator().get_next()\n",
    "# with tf.Session() as see:\n",
    "#     print(see.run(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 18, 16), (?, 1)), types: (tf.float64, tf.float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "SHUFFLE_BUFFER_SIZE = 5\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset\n",
    "val_dataset=val_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset\n",
    "# test_dataset=test_dataset.shuffle(2).batch(2)\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               73984     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 117,315\n",
      "Trainable params: 117,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 288 steps, validate on 6 steps\n",
      "Epoch 1/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 124533482.3805 - acc: 0.0208 - val_loss: 3.3140 - val_acc: 0.0333\n",
      "Epoch 2/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 64352010.3805 - acc: 0.0153 - val_loss: 3.3145 - val_acc: 0.0333\n",
      "Epoch 3/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 160120781.9357 - acc: 0.0181 - val_loss: 3.3150 - val_acc: 0.0333\n",
      "Epoch 4/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 120893880.6028 - acc: 0.0208 - val_loss: 3.3154 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 131811850.3809 - acc: 0.0264 - val_loss: 3.3156 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 147309194.3811 - acc: 0.0278 - val_loss: 3.3159 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 131197389.9363 - acc: 0.0354 - val_loss: 3.3159 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 123907148.1583 - acc: 0.0250 - val_loss: 3.3158 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 80190597.0477 - acc: 0.0299 - val_loss: 3.3157 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 91240401.4917 - acc: 0.0521 - val_loss: 3.3156 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 98792895.7145 - acc: 0.0576 - val_loss: 3.3156 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 158257802.3817 - acc: 0.0583 - val_loss: 3.3155 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 56166854.8253 - acc: 0.0757 - val_loss: 3.3153 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 139987817.4925 - acc: 0.0958 - val_loss: 3.3150 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 111894133.0480 - acc: 0.1139 - val_loss: 3.3147 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 69844842.3813 - acc: 0.1160 - val_loss: 3.3144 - val_acc: 0.0333\n",
      "Epoch 17/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 138583029.0492 - acc: 0.1167 - val_loss: 3.3141 - val_acc: 0.0333\n",
      "Epoch 18/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 126108216.6042 - acc: 0.1444 - val_loss: 3.3139 - val_acc: 0.0667\n",
      "Epoch 19/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 131765837.9381 - acc: 0.1576 - val_loss: 3.3136 - val_acc: 0.1000\n",
      "Epoch 20/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 115419395.2708 - acc: 0.1993 - val_loss: 3.3135 - val_acc: 0.1000\n",
      "Epoch 21/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 135261357.9383 - acc: 0.1993 - val_loss: 3.3135 - val_acc: 0.1667\n",
      "Epoch 22/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 88394613.0498 - acc: 0.2056 - val_loss: 3.3134 - val_acc: 0.1667\n",
      "Epoch 23/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 82556342.8271 - acc: 0.2347 - val_loss: 3.3134 - val_acc: 0.3333\n",
      "Epoch 24/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 114200339.2713 - acc: 0.2812 - val_loss: 3.3134 - val_acc: 0.3667\n",
      "Epoch 25/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 134799932.1602 - acc: 0.2854 - val_loss: 3.3133 - val_acc: 0.4000\n",
      "Epoch 26/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 61951573.0497 - acc: 0.3132 - val_loss: 3.3134 - val_acc: 0.4000\n",
      "Epoch 27/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 112332575.7164 - acc: 0.3389 - val_loss: 3.3133 - val_acc: 0.4000\n",
      "Epoch 28/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 115386673.4942 - acc: 0.3215 - val_loss: 3.3133 - val_acc: 0.4000\n",
      "Epoch 29/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 95769187.2730 - acc: 0.3556 - val_loss: 3.3134 - val_acc: 0.4000\n",
      "Epoch 30/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 86617014.8283 - acc: 0.3896 - val_loss: 3.3136 - val_acc: 0.4000\n",
      "Epoch 31/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 67690901.0509 - acc: 0.3924 - val_loss: 3.3137 - val_acc: 0.5000\n",
      "Epoch 32/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 99022790.8286 - acc: 0.4375 - val_loss: 3.3138 - val_acc: 0.5333\n",
      "Epoch 33/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 109615676.1625 - acc: 0.4458 - val_loss: 3.3141 - val_acc: 0.5667\n",
      "Epoch 34/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 122660819.2737 - acc: 0.4569 - val_loss: 3.3143 - val_acc: 0.6000\n",
      "Epoch 35/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 100822321.4961 - acc: 0.4542 - val_loss: 3.3147 - val_acc: 0.6000\n",
      "Epoch 36/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 166288931.2744 - acc: 0.5160 - val_loss: 3.3151 - val_acc: 0.6333\n",
      "Epoch 37/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 132493251.2752 - acc: 0.4965 - val_loss: 3.3155 - val_acc: 0.6667\n",
      "Epoch 38/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 98626186.3865 - acc: 0.5222 - val_loss: 3.3159 - val_acc: 0.6667\n",
      "Epoch 39/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 109011141.0539 - acc: 0.5153 - val_loss: 3.3163 - val_acc: 0.6667\n",
      "Epoch 40/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 59582433.4979 - acc: 0.5437 - val_loss: 3.3168 - val_acc: 0.6667\n",
      "Epoch 41/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 95864664.6101 - acc: 0.5347 - val_loss: 3.3173 - val_acc: 0.7000\n",
      "Epoch 42/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 135862241.4991 - acc: 0.5597 - val_loss: 3.3178 - val_acc: 0.7000\n",
      "Epoch 43/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 117814463.7220 - acc: 0.5889 - val_loss: 3.3182 - val_acc: 0.7333\n",
      "Epoch 44/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 58499576.6112 - acc: 0.5854 - val_loss: 3.3188 - val_acc: 0.7667\n",
      "Epoch 45/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 96499464.6116 - acc: 0.5778 - val_loss: 3.3193 - val_acc: 0.8333\n",
      "Epoch 46/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 64718236.1688 - acc: 0.6042 - val_loss: 3.3199 - val_acc: 0.8333\n",
      "Epoch 47/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 96365596.1695 - acc: 0.6160 - val_loss: 3.3205 - val_acc: 0.8333\n",
      "Epoch 48/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 112987061.0578 - acc: 0.6028 - val_loss: 3.3210 - val_acc: 0.8667\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 2ms/step - loss: 74514609.5036 - acc: 0.6306 - val_loss: 3.3215 - val_acc: 0.8667\n",
      "Epoch 50/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 65698401.5037 - acc: 0.6535 - val_loss: 3.3220 - val_acc: 0.9000\n",
      "Epoch 51/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 109170104.6143 - acc: 0.6674 - val_loss: 3.3226 - val_acc: 0.9000\n",
      "Epoch 52/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 151998623.7272 - acc: 0.6667 - val_loss: 3.3232 - val_acc: 0.9000\n",
      "Epoch 53/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 100289974.8397 - acc: 0.6625 - val_loss: 3.3238 - val_acc: 0.9000\n",
      "Epoch 54/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 74040340.1735 - acc: 0.6715 - val_loss: 3.3246 - val_acc: 0.9000\n",
      "Epoch 55/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 80851370.3961 - acc: 0.6576 - val_loss: 3.3252 - val_acc: 0.9333\n",
      "Epoch 56/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 134989928.6198 - acc: 0.6882 - val_loss: 3.3259 - val_acc: 0.9333\n",
      "Epoch 57/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 90764445.9535 - acc: 0.6597 - val_loss: 3.3265 - val_acc: 0.9333\n",
      "Epoch 58/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 80639957.9541 - acc: 0.6868 - val_loss: 3.3273 - val_acc: 0.9333\n",
      "Epoch 59/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 90093602.3987 - acc: 0.6979 - val_loss: 3.3281 - val_acc: 0.9333\n",
      "Epoch 60/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 96732998.8442 - acc: 0.7042 - val_loss: 3.3288 - val_acc: 0.9333\n",
      "Epoch 61/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 99961541.0683 - acc: 0.6917 - val_loss: 3.3297 - val_acc: 0.9333\n",
      "Epoch 62/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 121589359.7347 - acc: 0.7049 - val_loss: 3.3305 - val_acc: 0.9333\n",
      "Epoch 63/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 99121281.5131 - acc: 0.7090 - val_loss: 3.3314 - val_acc: 0.9333\n",
      "Epoch 64/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 167408981.0703 - acc: 0.7035 - val_loss: 3.3322 - val_acc: 0.9333\n",
      "Epoch 65/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 63417813.0710 - acc: 0.7076 - val_loss: 3.3329 - val_acc: 0.9667\n",
      "Epoch 66/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 124618803.2932 - acc: 0.7028 - val_loss: 3.3336 - val_acc: 0.9667\n",
      "Epoch 67/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 110199228.1833 - acc: 0.7382 - val_loss: 3.3344 - val_acc: 0.9667\n",
      "Epoch 68/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 110453941.0725 - acc: 0.7382 - val_loss: 3.3352 - val_acc: 0.9667\n",
      "Epoch 69/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 152159222.8517 - acc: 0.7292 - val_loss: 3.3360 - val_acc: 0.9667\n",
      "Epoch 70/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 106805853.0748 - acc: 0.7361 - val_loss: 3.3370 - val_acc: 0.9667\n",
      "Epoch 71/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 131536289.5185 - acc: 0.7389 - val_loss: 3.3377 - val_acc: 0.9667\n",
      "Epoch 72/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 102990389.0753 - acc: 0.7514 - val_loss: 3.3386 - val_acc: 0.9667\n",
      "Epoch 73/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 81820085.0761 - acc: 0.7542 - val_loss: 3.3395 - val_acc: 0.9667\n",
      "Epoch 74/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 90347891.2998 - acc: 0.7431 - val_loss: 3.3406 - val_acc: 0.9667\n",
      "Epoch 75/200\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 112740003.3013 - acc: 0.7479 - val_loss: 3.3415 - val_acc: 0.9667\n",
      "Epoch 76/200\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 82938997.0794 - acc: 0.7514 - val_loss: 3.3423 - val_acc: 0.9667\n",
      "Epoch 77/200\n",
      "288/288 [==============================] - 0s 1ms/step - loss: 93219773.9690 - acc: 0.7542 - val_loss: 3.3432 - val_acc: 0.9667\n",
      "Epoch 78/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 107578058.4154 - acc: 0.7556 - val_loss: 3.3441 - val_acc: 0.9667\n",
      "Epoch 79/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 80510560.6371 - acc: 0.7465 - val_loss: 3.3449 - val_acc: 0.9667\n",
      "Epoch 80/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 76174533.0819 - acc: 0.7569 - val_loss: 3.3459 - val_acc: 0.9667\n",
      "Epoch 81/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 83532565.0826 - acc: 0.7618 - val_loss: 3.3468 - val_acc: 0.9667\n",
      "Epoch 82/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 78109722.4167 - acc: 0.7639 - val_loss: 3.3477 - val_acc: 0.9667\n",
      "Epoch 83/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 93280806.8625 - acc: 0.7785 - val_loss: 3.3488 - val_acc: 0.9667\n",
      "Epoch 84/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 72347212.1974 - acc: 0.7632 - val_loss: 3.3497 - val_acc: 0.9667\n",
      "Epoch 85/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 89198876.1969 - acc: 0.7528 - val_loss: 3.3509 - val_acc: 0.9667\n",
      "Epoch 86/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 88545136.6427 - acc: 0.7757 - val_loss: 3.3520 - val_acc: 0.9667\n",
      "Epoch 87/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 120430701.9769 - acc: 0.7722 - val_loss: 3.3529 - val_acc: 0.9667\n",
      "Epoch 88/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 113931381.0897 - acc: 0.7701 - val_loss: 3.3537 - val_acc: 0.9667\n",
      "Epoch 89/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 72297286.8680 - acc: 0.7819 - val_loss: 3.3546 - val_acc: 0.9667\n",
      "Epoch 90/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 122889505.5349 - acc: 0.7576 - val_loss: 3.3555 - val_acc: 0.9667\n",
      "Epoch 91/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 93908979.3148 - acc: 0.7757 - val_loss: 3.3565 - val_acc: 0.9667\n",
      "Epoch 92/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 140934794.4261 - acc: 0.7826 - val_loss: 3.3574 - val_acc: 0.9667\n",
      "Epoch 93/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 89503357.9848 - acc: 0.7563 - val_loss: 3.3585 - val_acc: 0.9667\n",
      "Epoch 94/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 110866339.3162 - acc: 0.7819 - val_loss: 3.3596 - val_acc: 0.9667\n",
      "Epoch 95/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 109079199.7626 - acc: 0.7951 - val_loss: 3.3605 - val_acc: 0.9667\n",
      "Epoch 96/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 105070239.7629 - acc: 0.7924 - val_loss: 3.3615 - val_acc: 0.9667\n",
      "Epoch 97/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 148124575.7647 - acc: 0.7806 - val_loss: 3.3626 - val_acc: 0.9667\n",
      "Epoch 98/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 87643905.5438 - acc: 0.7965 - val_loss: 3.3637 - val_acc: 0.9667\n",
      "Epoch 99/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 84361186.4322 - acc: 0.8021 - val_loss: 3.3647 - val_acc: 0.9667\n",
      "Epoch 100/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 70531699.3226 - acc: 0.7944 - val_loss: 3.3657 - val_acc: 0.9667\n",
      "Epoch 101/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 98428863.7669 - acc: 0.8153 - val_loss: 3.3668 - val_acc: 0.9667\n",
      "Epoch 102/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 73564991.7699 - acc: 0.8062 - val_loss: 3.3682 - val_acc: 0.9667\n",
      "Epoch 103/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 144907235.3255 - acc: 0.8104 - val_loss: 3.3693 - val_acc: 0.9667\n",
      "Epoch 104/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 45342906.4386 - acc: 0.8111 - val_loss: 3.3704 - val_acc: 0.9667\n",
      "Epoch 105/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 126189285.5488 - acc: 0.8278 - val_loss: 3.3715 - val_acc: 0.9667\n",
      "Epoch 106/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 83713583.7747 - acc: 0.8167 - val_loss: 3.3724 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 76361357.1071 - acc: 0.8250 - val_loss: 3.3733 - val_acc: 0.9667\n",
      "Epoch 108/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 50227353.3301 - acc: 0.8292 - val_loss: 3.3743 - val_acc: 0.9667\n",
      "Epoch 109/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 100293292.2210 - acc: 0.8236 - val_loss: 3.3753 - val_acc: 0.9667\n",
      "Epoch 110/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 87929699.3329 - acc: 0.8083 - val_loss: 3.3764 - val_acc: 0.9667\n",
      "Epoch 111/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 136560557.9994 - acc: 0.8306 - val_loss: 3.3774 - val_acc: 0.9667\n",
      "Epoch 112/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 68231174.8911 - acc: 0.8410 - val_loss: 3.3784 - val_acc: 0.9667\n",
      "Epoch 113/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 90346445.1144 - acc: 0.8278 - val_loss: 3.3793 - val_acc: 0.9667\n",
      "Epoch 114/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 38286510.8919 - acc: 0.8250 - val_loss: 3.3802 - val_acc: 0.9667\n",
      "Epoch 115/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 60002044.2267 - acc: 0.8222 - val_loss: 3.3812 - val_acc: 0.9667\n",
      "Epoch 116/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 104181231.7841 - acc: 0.8222 - val_loss: 3.3821 - val_acc: 0.9667\n",
      "Epoch 117/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 45483397.1179 - acc: 0.8306 - val_loss: 3.3832 - val_acc: 0.9667\n",
      "Epoch 118/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 53350574.0069 - acc: 0.8361 - val_loss: 3.3840 - val_acc: 0.9667\n",
      "Epoch 119/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 90795985.5639 - acc: 0.8250 - val_loss: 3.3848 - val_acc: 0.9667\n",
      "Epoch 120/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 128021683.3444 - acc: 0.8396 - val_loss: 3.3859 - val_acc: 0.9667\n",
      "Epoch 121/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 114001187.3410 - acc: 0.8521 - val_loss: 3.3869 - val_acc: 0.9667\n",
      "Epoch 122/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 77103676.2332 - acc: 0.8375 - val_loss: 3.3878 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 45024998.9011 - acc: 0.8507 - val_loss: 3.3888 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 55320054.0125 - acc: 0.8590 - val_loss: 3.3897 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 96299208.6814 - acc: 0.8569 - val_loss: 3.3904 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 72841530.4595 - acc: 0.8722 - val_loss: 3.3914 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 55599197.5712 - acc: 0.8625 - val_loss: 3.3922 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 72159146.4600 - acc: 0.8590 - val_loss: 3.3932 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 130791411.3494 - acc: 0.8625 - val_loss: 3.3943 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 105173354.4642 - acc: 0.8528 - val_loss: 3.3951 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 68290371.3509 - acc: 0.8847 - val_loss: 3.3961 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 69372991.7977 - acc: 0.8757 - val_loss: 3.3971 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 121744348.2456 - acc: 0.8722 - val_loss: 3.3980 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 97437086.0227 - acc: 0.8799 - val_loss: 3.3987 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 45053989.1341 - acc: 0.8813 - val_loss: 3.3997 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 123906597.1353 - acc: 0.8813 - val_loss: 3.4005 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 82736067.3578 - acc: 0.8792 - val_loss: 3.4012 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 43222429.1377 - acc: 0.8771 - val_loss: 3.4021 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 130601507.3579 - acc: 0.8854 - val_loss: 3.4030 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 78644949.1396 - acc: 0.8792 - val_loss: 3.4042 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 104993199.8045 - acc: 0.8868 - val_loss: 3.4052 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 74155382.9192 - acc: 0.8875 - val_loss: 3.4060 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 26188653.1435 - acc: 0.8833 - val_loss: 3.4071 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 50953262.9219 - acc: 0.8979 - val_loss: 3.4077 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 48263726.0314 - acc: 0.9104 - val_loss: 3.4087 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 85323094.9231 - acc: 0.8785 - val_loss: 3.4098 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 78034810.4792 - acc: 0.8938 - val_loss: 3.4109 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 132417548.2580 - acc: 0.9035 - val_loss: 3.4120 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 82604991.8167 - acc: 0.9028 - val_loss: 3.4132 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 98092521.5916 - acc: 0.9118 - val_loss: 3.4139 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 53235989.1525 - acc: 0.9222 - val_loss: 3.4148 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 132513603.3746 - acc: 0.9056 - val_loss: 3.4157 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 73299895.8210 - acc: 0.9076 - val_loss: 3.4167 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 88907576.7088 - acc: 0.9125 - val_loss: 3.4177 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 100981160.7112 - acc: 0.9194 - val_loss: 3.4189 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 81778869.1534 - acc: 0.9194 - val_loss: 3.4201 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 100039850.4911 - acc: 0.9174 - val_loss: 3.4214 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 115913577.6037 - acc: 0.9229 - val_loss: 3.4223 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 77483733.1585 - acc: 0.9299 - val_loss: 3.4235 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 70443324.2703 - acc: 0.9160 - val_loss: 3.4246 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 58119142.9385 - acc: 0.9375 - val_loss: 3.4256 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 47715418.4950 - acc: 0.9312 - val_loss: 3.4268 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 57873546.4983 - acc: 0.9326 - val_loss: 3.4275 - val_acc: 1.0000\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 2ms/step - loss: 58904918.0516 - acc: 0.9236 - val_loss: 3.4284 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 41812002.4997 - acc: 0.9333 - val_loss: 3.4295 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 33843023.8312 - acc: 0.9208 - val_loss: 3.4305 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 39117525.1675 - acc: 0.9208 - val_loss: 3.4317 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 102518769.6102 - acc: 0.9319 - val_loss: 3.4331 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 92977454.0594 - acc: 0.9347 - val_loss: 3.4340 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 54672133.1679 - acc: 0.9354 - val_loss: 3.4351 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 115138375.6150 - acc: 0.9299 - val_loss: 3.4362 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 53677582.0615 - acc: 0.9375 - val_loss: 3.4376 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 84244408.7299 - acc: 0.9444 - val_loss: 3.4392 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 76463901.1728 - acc: 0.9389 - val_loss: 3.4404 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 75297310.0654 - acc: 0.9410 - val_loss: 3.4416 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 30075431.8458 - acc: 0.9403 - val_loss: 3.4431 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 75753130.5103 - acc: 0.9410 - val_loss: 3.4437 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 40790776.7370 - acc: 0.9521 - val_loss: 3.4453 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 29273213.1794 - acc: 0.9507 - val_loss: 3.4466 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 39498046.9619 - acc: 0.9431 - val_loss: 3.4473 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 26521463.8520 - acc: 0.9451 - val_loss: 3.4480 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 96183715.4077 - acc: 0.9458 - val_loss: 3.4492 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 18764169.8496 - acc: 0.9424 - val_loss: 3.4498 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 35744255.8547 - acc: 0.9542 - val_loss: 3.4520 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 30803087.8549 - acc: 0.9375 - val_loss: 3.4537 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 55483530.5204 - acc: 0.9500 - val_loss: 3.4552 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 55996860.3006 - acc: 0.9431 - val_loss: 3.4562 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 65137567.8603 - acc: 0.9507 - val_loss: 3.4575 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 25092024.0813 - acc: 0.9521 - val_loss: 3.4591 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 56012280.7498 - acc: 0.9576 - val_loss: 3.4605 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 23982443.4154 - acc: 0.9549 - val_loss: 3.4622 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 70191596.3074 - acc: 0.9542 - val_loss: 3.4636 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 36943772.3072 - acc: 0.9465 - val_loss: 3.4646 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 57046639.8674 - acc: 0.9618 - val_loss: 3.4661 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 15444015.4274 - acc: 0.9639 - val_loss: 3.4679 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 46622741.2048 - acc: 0.9514 - val_loss: 3.4684 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 121761123.4228 - acc: 0.9500 - val_loss: 3.4692 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 16796925.4266 - acc: 0.9583 - val_loss: 3.4704 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 51159102.0918 - acc: 0.9549 - val_loss: 3.4716 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "288/288 [==============================] - 0s 2ms/step - loss: 67448289.6552 - acc: 0.9583 - val_loss: 3.4731 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(64,(3,3),padding='same',activation='relu',input_shape=(18,29)),\n",
    "    tf.keras.layers.Flatten(input_shape=(18,16)),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150,input_shape=(18,29),activation='relu',return_sequences=True)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32,activation='relu'),\n",
    "    tf.keras.layers.Dense(3,activation='softmax')\n",
    "])\n",
    "# model.build(input_shape=(18,29,1))\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(train_dataset,epochs=200,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3iUVfbA8e8hhF6lKFIERUUICSV0FEREUMQVQUQsoIiC2NbGiqv+du0IoouyYgMsFAuCiiAoLib0Fqo0RQggEkroQuD+/jiTMAkzySQkM5nkfJ4nz2Te9847Z95Mzty57y3inMMYY0z4KxLqAIwxxuQOS+jGGFNAWEI3xpgCwhK6McYUEJbQjTGmgLCEbowxBYQl9AJMRCJE5JCI1MrNsqEkInVFJNf72opIRxHZ4nV/vYhcHkjZHDzXeyLyVE4fb4w/RUMdgDlNRA553S0F/AWc9Ny/1zn3SXaO55w7CZTJ7bKFgXPu0tw4joj0B25zzrX3Onb/3Di2MRlZQs9HnHNpCdVTA+zvnJvtr7yIFHXOpQQjNmOyYu/H0LMmlzAiIs+LyCQRmSAiB4HbRKSViCwQkf0islNE3hSRSE/5oiLiRKS25/7Hnv3fichBEZkvInWyW9azv4uIbBCRZBH5j4jEi0hfP3EHEuO9IrJJRPaJyJtej40QkddFZI+IbAY6Z3J+nhaRiRm2vSUiIzy/9xeRdZ7Xs9lTe/Z3rEQRae/5vZSIfOSJbQ3Q1Mfz/uo57hoR6ebZ3hAYBVzuac5K8jq3z3k9/j7Pa98jIl+JSLVAzk12znNqPCIyW0T2isgfIvKE1/P803NODojIEhE531fzlojEpf6dPedzrud59gJPi8jFIjLH81qSPOetvNfjL/C8xt2e/W+ISAlPzJd5lasmIkdEpJK/12t8cM7ZTz78AbYAHTNsex44DlyPfhiXBJoBLdBvWxcCG4DBnvJFAQfU9tz/GEgCYoFIYBLwcQ7KVgUOAjd49v0dOAH09fNaAolxKlAeqA3sTX3twGBgDVADqATM1betz+e5EDgElPY69p9ArOf+9Z4yAnQAjgLRnn0dgS1ex0oE2nt+fw34CagIXACszVD2ZqCa529yqyeGcz37+gM/ZYjzY+A5z++dPDE2AkoAbwM/BnJusnmeywO7gIeA4kA5oLln3z+ABOBiz2toBJwD1M14roG41L+z57WlAAOBCPT9eAlwFVDM8z6JB17zej2rPeeztKd8G8++McALXs/zKDAl1P+H4fYT8gDsx88fxn9C/zGLxz0GfOb53VeS/q9X2W7A6hyUvQv42WufADvxk9ADjLGl1/4vgcc8v89Fm55S912bMclkOPYC4FbP712ADZmU/Qa43/N7Zgl9q/ffAhjkXdbHcVcD13l+zyqhjwNe9NpXDr1uUiOrc5PN83w7sMRPuc2p8WbYHkhC/zWLGHoAiz2/Xw78AUT4KNcG+A0Qz/0VQPfc/r8q6D/W5BJ+tnnfEZF6IvKt5yv0AeBfQOVMHv+H1+9HyPxCqL+y53vH4fQ/MNHfQQKMMaDnAn7PJF6AT4Hent9vBdIuJItIVxFZ6Gly2I/WjjM7V6mqZRaDiPQVkQRPs8F+oF6AxwV9fWnHc84dAPYB1b3KBPQ3y+I81wQ2+YmhJprUcyLj+/E8EZksIts9MYzNEMMWpxfg03HOxaO1/bYiEgXUAr7NYUyFliX08JOxy947aI2wrnOuHPAMWmPOSzvRGiQAIiKkT0AZnU2MO9FEkCqrbpWTgI4iUgNtEvrUE2NJ4HPgJbQ5pALwfYBx/OEvBhG5EBiNNjtU8hz3F6/jZtXFcgfajJN6vLJo0872AOLKKLPzvA24yM/j/O077ImplNe28zKUyfj6XkF7ZzX0xNA3QwwXiEiEnzjGA7eh3yYmO+f+8lPO+GEJPfyVBZKBw56LSvcG4Tm/AZqIyPUiUhRtl62SRzFOBh4WkeqeC2RPZlbYObcLbRb4EFjvnNvo2VUcbdfdDZwUka5oW2+gMTwlIhVE++kP9tpXBk1qu9HPtv5oDT3VLqCG98XJDCYAd4tItIgURz9wfnbO+f3Gk4nMzvM0oJaIDBaRYiJSTkSae/a9BzwvIheJaiQi56AfZH+gF98jRGQAXh8+mcRwGEgWkZpos0+q+cAe4EXRC80lRaSN1/6P0CaaW9HkbrLJEnr4exS4E71I+Q5aQ81TnqTZCxiB/oNeBCxHa2a5HeNo4AdgFbAYrWVn5VO0TfxTr5j3A48AU9ALiz3QD6ZAPIt+U9gCfIdXsnHOrQTeBBZ5ytQDFno9dhawEdglIt5NJ6mPn4E2jUzxPL4W0CfAuDLye56dc8nA1cBN6EXYDUA7z+5hwFfoeT6AXqAs4WlKuwd4Cr1AXjfDa/PlWaA5+sEyDfjCK4YUoCtwGVpb34r+HVL3b0H/zsedc/Oy+doNpy9AGJNjnq/QO4AezrmfQx2PCV8iMh690PpcqGMJRzawyOSIiHRGv0IfQ7u9paC1VGNyxHM94gagYahjCVfW5GJyqi3wK/pVvDPwN7uIZXJKRF5C+8K/6JzbGup4wpU1uRhjTAFhNXRjjCkgQtaGXrlyZVe7du1QPb0xxoSlpUuXJjnnfHYTDllCr127NkuWLAnV0xtjTFgSEb+jpa3JxRhjCghL6MYYU0BYQjfGmALCEroxxhQQltCNMaaAyDKhi8gHIvKniKz2s188S1BtEpGVItIk98M0xhiTlUBq6GPJZB1HdFWYiz0/A9DZ8YwxxgRZlv3QnXNzxbNwsB83AOM9U20u8MwZXc05tzOXYjSmUElKgtGj4cSJUEdi8sr110OzZrl/3NwYWFSd9MtQJXq2nZHQPRPkDwCoVSurhWeMKZzefReeeQYkr9edMiFz/vn5N6H7etv5nPHLOTcGnTyf2NhYmxXMGB/i4uCyy2Dt2lBHYsJNbvRySST9eos10MUOjDHZdOoUzJsHbduGOhITjnIjoU8D7vD0dmkJJFv7uTE5s24d7N8PbdpkXdaYjLJschGRCUB7oLKIJKJrBkYCOOf+C0wHrgU2AUeAfnkVrDEFXVyc3lpCNzkRSC+X3lnsd8D9uRaRMYVYfDycey5cdFGoIzHhyNYUNTny3nvw1FNgC17lrv37oVs36+ESto4dg/nzoVo1qFcv6E9vCd3kyCefQPHimnxM7hGBftZomb85B0eOQOnS6bcvWwZXXgkHDujXrF9+gQoVdN/KlXDJJfpPM2sWNGkClSvnemiW0E22nTgBCxfCvffC66+HOhpjApT6dTKzrz87dsDmzXD55b73JyVB//4wdSq0aqXl6taFvn31K2uxYvDf/8KgQTB0KLz1FgwfDo89BlWrQpUqsGYNvPIKPPFErr9ES+gm25Yvh6NH7cKdCSMnT0KXLtontFEjGDVKb70dPw7XXKNdjTZt0pr2N99oMv7hB03Qa9Zo2fvu06aVkSP1cePG6QWQYcO0prNuHbz5JiQk6PauXfUDJSkJxo6F3plemswxS+gm26wnhsmXEhNhyhS4664zm0NGjNCmjp49NcF26KAJevlyfdw552jzyOrVUKQIvPwyzJkDGzboMRMStCb+0ENw++3QsKEe1zlN6n//u9bABw7U7c8/r/vmz4c774QxY7T2nsfEheiqVmxsrLM1RcPTTTfBihX6zdSYs3bsGEREQGRkzo+xYQN07AjbtsGll2pzyPLlULGi1s4XLNAJVD7/HH77Tdu6t27Vi5d168L69fDnn9CnDxQtqjVugPvv1yaU2FiYMeN0m3hGU6ZApUpwxRU5fw0BEpGlzrlYn/ssoRdea9fmbHj5oEHQuTOMH5/7MZlCqEsXbcqYMgXOO09ryyVLwt69+lO37umyycmalOfO1eaLm26Cpk2hUyetET//PPz733D4MLRoAYcOaY27bl1tt069EHnoEPzxh/YPFdEPlVmztOb+++8QFQU33wwTJ8KWLRpXiRIhOT0ZWUI3Z3AOatTQa0A5MW4c3HFH7sZkCgnn4J13dIaqmBioXVtrxSkpur96dfj+e+jeHbZv1/boGjU0scbGwp49WrMuVUq/JhYtqgl31iztKuic/hQ5i4Hwq1bBxRfnmyTuLbOEbm3ohdTmzZrM/+//9P8mOyIjtQeWMWmOHQss+aWk6Fe8d9+FsmXh7rt1+7x5egGyTBmtYcfE6PaICG23njxZaxDHj+tFnNatNWmPGAHffQcffAAXXKCPETn7jvypbeThxjkXkp+mTZs6Ezpjx2o1ZtWqUEdiwt7Klc6VK+fc8OHpt5886dybbzo3aJBzjz7q3MSJzrVqpW+8e+5xLiJCf7/88vSPmzHDuVKlnBs1yrkXXtAyNWro7dixwXtd+RSwxPnJq1ZDL6Ti4vT6Tv36oY7EhLXDh7Wt+cAB7RkycKC2fx8/rn2zJ0zQNvHDh+Gvv6BcOfj0U+22FxkJb78Nt92W/pjXXKNDZiMj9TgHD+oFzMaNrZ0vC5bQC6n4eP3WejbNjKYQSEzUJpJRoyDjojSnTukgm/Xr4dlntf3u/fe1lvDgg3qh86WX4MknNTEvXqwXIatV08c//7xepOzT58znTe3xUqyYHsMExBJ6IbRnj15nylgxMuYMgwfD119rN79HHtFtv/8Os2frUPeJE7Vm/sQT2q3vgQe0TM2a+riuXfV+8eJnTvJesaJ+CJhcYwm9APvf/7QSdOpU+u3JyXpriyiYTE2dqj+g3QQfeQR27dIRZdu36/b+/TWZi8B//gMffqhvrG7d9AKnCSpL6AXYO+/oQLXGjdNvL1lSu++2aBGauEwYOH5cRz9GRWmPkxkzdBKfm2/WvuE//qhNJ5deerpHSbNmebNQpgmYJfQCLD4errsOJk0KdSQmLPz+u7aZX3QRfPkl/PqrdgncuVOn13zqKa2pjx2rTTAm37FLYgXUtm3aMcCaVQqRadPg6afPbGMD7Sd+7NiZ5du107bwf/xDB/i0bQt16mjyvuIK7XGSOpz9tdd0VKb1NMm3rIZeQMXH661NoFWIDBum/VFFdHDO3r0wYIDWqnfv1jJ16sDVV+uIyiFDtGmleXOd76RfP22LmzxZh9e/+qoe68ILdVRn6kg0W30j37KEXkDFxemEc9HRoY7E5KqtW8/sPgg6n/GiRdpz5PnndZj8mjX606ePNqM4p10HJ0zQvt1RUfDZZ3pRs3597R4oou10Y8eeTtwi0KuXHuvaa4P5ak02WUIvYE6e1KbP//1PJ5wran/hguPTTzU5jxypw+G9LVqkte2JE/XT/J13dJj9V1/p5FfeTpzQmQgvvlg/AKZNO/O5MtbCR4zI3ddi8oS1oRcwQ4fqPCurV/tfdMWEoZQUHbwjAo8+CjNn6vaNG/Vn7lzd1769rpCzbZsugZYxmYMO2mneXJO5KVCs/lbAzJql3RSffNK+HRcIv/6qCdo5XUXno4/ghRd0/uIGDbQZpFw5bYaJjj6dpCtWtIRdCFkNvQA5eFAXnujaVZs8y5YNdUQm23bs0DlPQAfvXHWVzncyerROHdunjw4ueOkl/QP/4x9a4169OiiLK5j8zWroBcjChdpjzboqhqmtW7XWffXV2tOkWzddxGHRIp37+9xztVmlQgXtoTJkiD6uY0f9OnbddaGN34ScJfQCJC5OJ9tq2TLUkRic00/XiIjAH/PQQ7qSzpQpcM892j/8s8+yHn3ZoYPOdhiENStN/mZNLgVIfLw2o5YrF+pICrmTJ+GGG7QJJHUVHl+cOz3Y55tvtEfKM8/oYsNjx2pzy003BfaclswNVkMPaydP6vWxvXv1/rx5OjbEhNhLL+lMg6CrvQ8adGaZxEQdcblypfYNf/JJnRfl6af1Auejj8Ibb9ggHpMttqZoGFuyRL+Nlyql18UiI3Xelg4dQh1ZAXXokHYj+tvfNNFu365Tx1atqu3ZkZG66nbDhnDLLToHyooVOtVs48ank/Pq1dq9MLV2XrasLlj82WfQo4duO37cat3GJ1tTtIDaulVv4+LOnFHR5IF//EMXehg1ShcyvvVWTbwnT8K332rf8PHjNXGPHKlTzbZsqfOfNGig5c8/X+dJKVZM28jmzYO77tIy3s0rlsxNDlhCD2OpCd3XSHCTy5KSdDWeyEh47DG94NmokY7MXLhQl1QbNkzvX301VKmiP7//rjXvjz/WUV8AlSrBTz9pE8sll+iHQvv21rxizppdFA1jW7dqc8s554Q6kgJm7Fi9urxtm94/eFBnGjx6VOcFL1NGh81/951OdnXLLdCzJ7zyiibw3r1PH6tSJbjvPv0a9ccfOjhoyxadRwU0id97ryZ3Y85SQG3oItIZeAOIAN5zzr2cYX8tYBxQwVNmiHNuembHtDb0s9ezJ6xapSO8TS5xTieq+uUXTeo1a2pzCmjPla++0pkLy5TRlUJSrVmjbefFi2tTi3U1MnnkrNrQRSQCeAu4GkgEFovINOfcWq9iTwOTnXOjRaQ+MB2ofdaRm0z5m3jP5MC2bfpTtKgm8z59tPlk61ZtKqlXTy+GgjalZNSgATz+uPY7t2RuQiSQNvTmwCbn3K8AIjIRuAHwTugOSH0Xlwd25GaQxretW21wYK6YMEGbPQ4e1Fp5iRLw1lt68bJatcDnRHnllbyN05gsBNKGXh3Y5nU/0bPN23PAbSKSiNbOH/B1IBEZICJLRGTJ7tQJ902O/PWXNsnWrBnqSMJM37560h5/XBP4kiXa+yQ6WlfnWblSa+Lly2vTi01wZcJIIDV0X5feMza89wbGOueGi0gr4CMRiXLOpVsLyzk3BhgD2oaek4CNSl103ZpcsuHHH2HcOL0gOWKEtoUnJ2vSnj5d279fe00vchoThgKpoScC3vXAGpzZpHI3MBnAOTcfKAFUzo0AjW/WZdGHRYu0l0mqpCSdpfDECR2C/+CD2itl8WJtThk3Ti9yPvSQtnsXL67t5RddFLrXYMxZCCShLwYuFpE6IlIMuAXIuMTJVuAqABG5DE3o1qaSh1J71BWKhJ6SAoMHayJOdeoU7Nlz+v769Tp3yr336v3ffoPWrXXY/ZdfanfDNWu0nbtECR1iX7++jtJ8wGcLoTFhJ8uE7pxLAQYDM4F1aG+WNSLyLxHp5in2KHCPiCQAE4C+LlRzChQSqTX0GjVCG0dQjBunFykHD9ZuhQD/+Q9UrqyTv3/+OfTvrxcWfvgB9u07PfVs+fLa7XDaNE3eN9ygjy9eXAf3LFxoHflNgWFzuYSpe+/VWVb//DPUkeSxo0d1NGVysl7E/OYb7drTuLG2gR8/fnpF+4EDtYnlzjv1Q2DsWJ17ZeZMHeHZpo2O2jQmjGXWD91GioapDRu0ObjAOnFCh9pff73OTPjFF/qCn3kG1q3TSa8efVRX+ImP1wUhRo3SbobjxumEWbfcojX4pCSdKOv660P9qozJU5bQw9CJE9pS0KpVqCPJJc5pwj16VO8nJOhkVf3761D5//s/nR/lpZd00YfUxNyjhw4Eat1ah80WKQI33qj77r1Xm1WuuUYH+xQpYousmgLPEnoYWrFCc1+bNqGOJECLF+vc3w0baoL23j5wIJx3ns5CeMklmsyvvVZr1V99pRc3n3lGy/fqpf3IN2/WF++rE37//jqn8MCBer9iRV2irUMHbXM3pgCz2RbDUFyc3oZFQt+6VWcSjIzUaWZvv137eg8Zoi+kZEm9gNm4sa7W0bSp1roXLNDZDDMaNUovHNx9t+/na9xYuy96++KLXH9ZxuRHltDDUHy8Nieff36oIwnAgw/qbUKCrlbfu7euYn3++Tpn+F13ae8TgJgYnRN8xAjfyRygdOnTk2UFqnTpnMdvTBixhB5mnNOE3rFjqCPx8tNP2s4tAn//u16IjI/XroZTp8Krr8IFF+jPqlXaM+Wf/zxzEqvOnbXLoS3uYEyOWEIPM7/+qnO4tG0b6ki8vPuutodXraoXKu+/H15/XfuAP/AAPPzw6bIvvJD5sSyZG5NjdlE0zMTH622+aj9fsEBr14sW6UinESO0lp6YCG++qe3nxpg8Zwk9zMTFQYUKOmo9X9i9W782tGihvUhmz9bBPV9+aW3XxgSZNbmEmfh47XZdJL98FC9cqLctW+pt7dq65JoxJujyS1owAdi7F9auzYfNLRER2t3QGBNSVkMPI/Pm6W3QLog6d3ol+gULtO94UhKMGaP3t2/XNqCYGF2t2hgTUpbQw4BzWjOfOlWvLzZrFoQnHD1aR2hOmqRD6Nu107U0IyJOf0WIiNDBQoMG5XFAxphAWEIPAz/+eLrfeZs26RebzxNPPKGjOSMjYcAAHfhTvTqsXq0TyYwYoQFVqwYvv6zD7Y0xIWcJPQz88IOOhv/88yA0VX/zjSbz++7TUZtXX63bJ006PRDIuy/52LF5HJAxJlCW0MNAfDw0aXJ6bYZctWaNNplMmaJNLXffrQsmjxypTS2PP65T1PbsmQdPbozJTZbQ87njx3W8Turkgbnu009h7lxdJPngQZ346ttvNZmDDts3xoQFS+j53LJlcOxYHvZsmTNHb2fN0nlU6tSxLojGhCnrh57P5clQ/99/hzfe0GXdUhdenjlTr7526XK6q6IxJqxYDT0f+f13ePFF7UiSKi4OLroIzj03hwc9eVJX7/nzT11A4s47dQKtDRvg558hJUWXaps4Uct36XLWr8MYExqW0PORDz7QMTsZF+I5q/bzjz7StTkvvRRmzIDhw3VGw3r1dOGHyEh47jlN6MWKwZVXns1LMMaEkCX0fCQ+XhfcWbYslw54+DAMHaoTZ82fr18Bhg2DTp102beWLaF5c032MTE6U6JNqGVM2LKEnk+kpOho+n79cvGgo0drl8PJk7VdvHZtXXQi1RtvaE0d4PvvbZpbY8KcJfR8IiFBK9S52pvl44+hVSv/V1RTl4cDXZzCGBPWrJdLPpHrvVnWr9dPiV69cumAxpj8zhJ6PhEXB7VqaTN2rkhtZunRI5cOaIzJ7yyh5wOpCz/nanPL5Ml6wOrVc/Ggxpj8zBJ6PrBli167zLXmls2bdWZEq50bU6hYQs8Hcr39fOZMvbVBQsYUKpbQ84G4OJ2ZNirqLA6SOk95YqIOIKpTB+rWzbUYjTH5X0AJXUQ6i8h6EdkkIkP8lLlZRNaKyBoR+TR3wyzY4uO1d2FExFkcZNgwePRR6NtXJ9y65hqbk8WYQibLfugiEgG8BVwNJAKLRWSac26tV5mLgX8AbZxz+0TEOjUHaN8+be4+q96Fa9fC//2fXgD94Qfd1rlzrsRnjAkfgdTQmwObnHO/OueOAxOBjEst3AO85ZzbB+Cc+zN3wyyYvv1Wl+2Es2w/f+45HbK/cCHUr6/LG9mcLMYUOoGMFK0ObPO6nwi0yFDmEgARiQcigOecczMyHkhEBgADAGrVqpWTeAuM5GTo1g1OnYLKlXVKlRw5flzbzHv10hr655/DL7+cXi7OGFNoBFJD99UQ6zLcLwpcDLQHegPviUiFMx7k3BjnXKxzLrZKlSrZjbVAmT9fk/m0aXodM1tzYm3cqJO/gF5RPXgQrrtO7192Gdx4Y67Ha4zJ/wJJ6ImA94SuNYAdPspMdc6dcM79BqxHE7zxIy5OL4JeeeXp1d4CMn++zmt+ySU6V8u33+q0tx075lmsxpjwEEiTy2LgYhGpA2wHbgFuzVDmK7RmPlZEKqNNML/mZqAFTXw8NGoEZcpk84HTp+snQeXKcPvteoD27XNwIGNMQZNlDd05lwIMBmYC64DJzrk1IvIvEenmKTYT2CMia4E5wOPOuT15FXS4O3FCr1/m6ELo7Nna4B4fDzfdBIcOnW5uMcYUagFNn+ucmw5Mz7DtGa/fHfB3z4/JwvLlcPRoDuZuSU6GRYvgqad07vIJE7QRvmvXPInTGBNebD70EIiL09ts19D/9z+9kpraXh4ZqbV0Y4zBhv6HRHy8jsw///xsPnD2bChVSpeOM8aYDCyhB1nqVLnZrp07B999B+3aZbNbjDGmsLCEHmSbN8OuXTloP1+7FjZt0tFIxhjjg7WhB1m2p8r99VcoUQKmTtX7ltCNMX5YQg+yuDioUEGnXAnItddq18Ty5bW7YrYb3o0xhYU1uQRZfDy0bg1FAjnzv/2miz1v365NLjdknBPNGGNOs4QeRHv2wLp12WhumTVLb0eMgIYNoXfvPIvNGBP+LKEH0bx5epvlBVHnmfvs+++hZk14+GFYuVL7OhpjjB/Whh5EcXE6FqhZsywKXn65zs2ycCF0724rDxljAmIJPYji46FpUyhZ0sfOQYN0kq1GjU53hQHo1ClY4Rljwpw1uQTJsWOweLGf9vMTJ+Cdd2DoUJ2bBeDdd6FnT+jSJahxGmPClyX0IFm6VBcX8pnQExN1jpZNm+CVV/QCaP/+MHmyrTxkjAmYJfQgyXRA0ZYtp39PSoLrrw9GSMaYAsYSepBs2ADnnQdVq/rYmZrQUxO5JXRjTA7YRdEgSUoCv8uobtmiI43GjIFvvoEWGdfgNsaYrFlCD5KkJF01zqctW6BGDa3C9+8fzLCMMQWINbkEyZ49WST02rWDGI0xpiCyhB4kmdbQf/vNErox5qxZQg+Ckydh714/Cf34cZ18yxK6MeYsWUIPgv37tZt5pUo+dqb2QbeEbow5S5bQgyApSW/PqKHv3av9GcESujHmrFkvlyDwmdA3b4bGjeHoUb1vCd0Yc5YsoQfBnj16m5bQU1Lg9tu17/lNN2mBmjVDFp8xpmCwhB4EZ9TQn34a5s+HTz6BW28NWVzGmILF2tCDIF1CHz5cJ+C67z5L5saYXGUJPQiSkqBECSj11z544gldtGLUqFCHZYwpYCyhB0FSknZZlDWrtYviPfdARESowzLGFDCW0IMgbdj/6tW6ISoqpPEYYwomS+hBkDbsf/VqXbCievVQh2SMKYAsoQdBuoQeFWWLPhtj8kRACV1EOovIehHZJCJDMinXQ0SciMTmXojhLykJKldypxO6McbkgSwTuohEAG8BXYD6QG8Rqe+jXFngQWBhbgcZzlJSYN8+qFzysA71t4RujMkjgQwsag5scs79CiAiE4EbgLUZyv0beBV4LFcjDANPPgnjx/ved+oUOAeVjoWdQs0AABtISURBVG3XDZbQjTF5JJCEXh3Y5nU/EUi3RpqINAZqOue+ERG/CV1EBgADAGrVqpX9aPMh5+D99+H886FVK99likU6upeYrncaNAhecMaYQiWQhO7rCp5L2ylSBHgd6JvVgZxzY4AxALGxsS6L4mFh/XrtlvjKK3D33T4KHD0KV14JCxdqMve5SrQxxpy9QC6KJgLeM0fVAHZ43S8LRAE/icgWoCUwrbBcGI2L09s2bfwUmD9fk/kzz5wubIwxeSCQhL4YuFhE6ohIMeAWYFrqTudcsnOusnOutnOuNrAA6OacW5InEecz8fE6CvTSS/0UmDdPbx95BCpUCFpcxpjCJ8uE7pxLAQYDM4F1wGTn3BoR+ZeIdMvrAPO7+HitnfvtWh4fr00tlsyNMXksoOlznXPTgekZtj3jp2z7sw8rPOzaBRs36tQsPp06pU0uvXoFNS5jTOFkI0XPQmprit/283XrIDkZWrcOWkzGmMLLEvpZiIuD4sWhaVM/BVIzviV0Y0wQWEI/C/Hx0KyZJnW/BapUgbp1gxqXMaZwsoSeQ0eOwLJlmTS3gNbQW7e2ybiMMUFhCT2HFi+GEyegbVs/BXbv1ium1txijAkSWyQ6h+Lj9faMfP3WW7B9O7Rs6aeAMcbkDUvoORQfD/XrwznneG10Dl5+WRP6jTdCZGQmV0yNMSZ3WZNLDv32G1x2WYaNq1ZBYqIm9i+/1GResmRI4jPGFD6W0HPo8GEoWzbDxumesVdduuitNbcYY4LIEnoOHToEZcpk2Dh9OjRuDMOGaV/Gzp1DEpsxpnCyNvQcOiOh79un3RSHDNG5W/bvhxIlQhafMabwsRp6Dhw/rj+lS3ttnDULTp6Ea6/V+5bMjTFBZgk9Bw4f1tt0NfTp07XLS4sWPh9jjDF5zRJ6Dhw6pLdpCf3UKfjuO7jmGoiICFlcxpjCzRJ6DpyR0JcuhT//PN3cYowxIWAJPQfOSOjTp+t8LddcE7KYjDHGernkQFpCL3kSHh8Cb76ps3RVqRLawIwxhZrV0HMg7aLo8p/htdegZ0+YNCm0QRljCj2roedAWg1961ptahkzBkqVCm1QxphCz2roOZCW0Leshtq1LZkbY/IFS+g5kJrQS29O0FGhxhiTD1hCz4G0hL4xQefQNcaYfMASeg4cOgTFIk9R7MRhq6EbY/INS+g5cOgQlCl+Qu9YQjfG5BOW0HPg0CEoE3FU79SrF9pgjDHGwxJ6Dhw+DGXcIahTJ8OUi8YYEzqW0HPg0CEoc3yPNbcYY/IVS+g5cGjvX5Q5lgRXXBHqUIwxJo0l9Bw49MdhynAIOnYMdSjGGJPGEnoOHNp3nDLFTkBMTKhDMcaYNAEldBHpLCLrRWSTiAzxsf/vIrJWRFaKyA8ickHuh5pPOMehw0LpGhWgiH0eGmPyjywzkohEAG8BXYD6QG8RyTg8cjkQ65yLBj4HXs3tQPONtWs5dKoUZS48N9SRGGNMOoFUMZsDm5xzvzrnjgMTgRu8Czjn5jjnjnjuLgBq5G6YIXbyJCxaBN9+y6nrrucwpSnToOB+CTHGhKdAEnp1YJvX/UTPNn/uBr7ztUNEBojIEhFZsnv37sCjDLXx43Xx565dOXo8AkcRypxfLtRRGWNMOoEkdPGxzfksKHIbEAsM87XfOTfGORfrnIutEk6r+yxcCBUqwPffc/inxYDX8nPGGJNPBLLARSJQ0+t+DWBHxkIi0hEYCrRzzv2VO+HlEytWQKNGcPXVHPpVN1lCN8bkN4HU0BcDF4tIHREpBtwCTPMuICKNgXeAbs65P3M/zBA6eRJWrUrronjGAtHGGJNPZJnQnXMpwGBgJrAOmOycWyMi/xKRbp5iw4AywGciskJEpvk5XPjZtAmOHNEaOpbQjTH5V0BrijrnpgPTM2x7xuv3gjtkMiFBb62GbozJ52yR6KysWAFFi0L9+jz1FHzxhW62SRaNMfmNDXXMyooVcNllULw4b78Nx49D795w6aWhDswYY9KzhJ6Zo0dh8WKIieHAAUhOhoED4dNPoUSJUAdnjDHpWULPzPDhkJQE/fqxzTO0qlat0IZkjDH+WEL3Z9s2ePFF6NEDOnRg61bdbAndGJNfWUL3Z/RobTB/7TUAS+jGmHzPern4M2MGtG4NF+gkXFu3QkQEVKsW4rhMgXPixAkSExM5duxYqEMx+UiJEiWoUaMGkZGRAT/GErovu3bB8uXw/PNpm7Ztg+rVNakbk5sSExMpW7YstWvXRsTX1EmmsHHOsWfPHhITE6lTp07Aj7MmF1++/15vO3dO27R1qzW3mLxx7NgxKlWqZMncpBERKlWqlO1vbZbQfZk5E6pUgcaN0zZZQjd5yZK5ySgn7wlrckn1008waRIsW6bD/Xv0SFti7tQpSEyEmjUzP4QxxoSS1dAB1q2DDh3g44+hbFno1QueeCJt965dcOKE1dBNwbRnzx4aNWpEo0aNOO+886hevXra/ePHjwd0jH79+rF+/fpMy7z11lt88sknuRGy8cNq6ACjRkGxYrB5M1StesZu67JoCrJKlSqxYsUKAJ577jnKlCnDY489lq6Mcw7nHEX8LIz+4YcfZvk8999//9kHG2QpKSkULRo+adJq6Pv3w7hxOkGLj2Q+ejQ845lX0hK6yXMPPwzt2+fuz8MP5yiUTZs2ERUVxX333UeTJk3YuXMnAwYMIDY2lgYNGvCvf/0rrWzbtm1ZsWIFKSkpVKhQgSFDhhATE0OrVq34809dIuHpp59m5MiRaeWHDBlC8+bNufTSS5k3bx4Ahw8f5qabbiImJobevXsTGxub9mHj7dlnn6VZs2Zp8Tmni6ht2LCBDh06EBMTQ5MmTdiyZQsAL774Ig0bNiQmJoahQ4emixngjz/+oG7dugC899573HLLLXTt2pUuXbpw4MABOnToQJMmTYiOjuabb75Ji+PDDz8kOjqamJgY+vXrx/79+7nwwgtJSUkBYP/+/dSpU4eTJ0/m6G+QXZbQx46Fw4fhgQfO2HXokG5esECvj158cfDDMyaU1q5dy913383y5cupXr06L7/8MkuWLCEhIYFZs2axdu3aMx6TnJxMu3btSEhIoFWrVnzwwQc+j+2cY9GiRQwbNiztw+E///kP5513HgkJCQwZMoTly5f7fOxDDz3E4sWLWbVqFcnJycyYMQOA3r1788gjj5CQkMC8efOoWrUqX3/9Nd999x2LFi0iISGBRx99NMvXPX/+fD766CNmzZpFyZIlmTp1KsuWLWP27Nk88sgjACQkJPDKK6/w008/kZCQwPDhw6lQoQJt2rRJi+fTTz/l5ptvJiJI/Z3D57tEXnAO3n0XWraEJk3O2L1woS5YNHkyXHNNCOIzhY+nBptfXHTRRTRr1izt/oQJE3j//fdJSUlhx44drF27lvr166d7TMmSJenSpQsATZs25eeff/Z57O7du6eVSa1Jx8XF8eSTTwIQExNDgwYNfD72hx9+YNiwYRw7doykpCSaNm1Ky5YtSUpK4vrrrwd0YA7A7NmzueuuuyhZsiQA55xzTpavu1OnTlSsWBHQD54nn3ySuLg4ihQpwrZt20hKSuLHH3+kV69eacdLve3fvz9vvvkmXbt25cMPP+Sjjz7K8vlyS+FO6IsWwdq1mtR9iIsDEc33xhRGpb0m/t+4cSNvvPEGixYtokKFCtx2220++0kXK1Ys7feIiIi05oeMihcvfkaZ1KaTzBw5coTBgwezbNkyqlevztNPP50Wh6+ufs45n9uLFi3KqVOnAM54Hd6ve/z48SQnJ7Ns2TKKFi1KjRo1OHbsmN/jtmvXjsGDBzNnzhwiIyOpV69elq8ptxTuJpf334dSpeDmm33ujo+H6GgoXz7IcRmTDx04cICyZctSrlw5du7cycyZM3P9Odq2bcvkyZMBWLVqlc8mnaNHj1KkSBEqV67MwYMH+cKz6kzFihWpXLkyX3/9NaBJ+siRI3Tq1In333+fo0ePArB3714AateuzdKlSwH4/PPP/caUnJxM1apVKVq0KLNmzWL79u0AdOzYkYkTJ6YdL/UW4LbbbqNPnz7069fvrM5HdhXehH7oEEycCD17QrlyZ+xOSYH586FNmxDEZkw+1KRJE+rXr09UVBT33HMPbfLgn+OBBx5g+/btREdHM3z4cKKioiifoUZVqVIl7rzzTqKiorjxxhtp0aJF2r5PPvmE4cOHEx0dTdu2bdm9ezddu3alc+fOxMbG0qhRI15//XUAHn/8cd544w1at27Nvn37/MZ0++23M2/ePGJjY/nss8+42HMxLTo6mieeeIIrrriCRo0a8fjjj6c9pk+fPiQnJ9OrV6/cPD1ZS+2OFOyfpk2bupAaNco5cG7ePJ+7ly3T3Z98EuS4TKGzdu3aUIeQb5w4ccIdPXrUOefchg0bXO3atd2JEydCHFX2TZgwwfXt2/esj+PrvQEscX7yauFsQz95Ui8+tWwJrVr5LBIfr7dt2wYxLmMKuUOHDnHVVVeRkpKCc4533nknrPqBAwwcOJDZs2en9XQJpvA6U7lh5Ur47jvYtEkXsPAjLg5q1LC+58YEU4UKFdLatcPV6NGjQ/bchSehHzwIDz6o/c4BGjSAG2/0WdQ5TeiXXx688Iwx5mwVnouid90F48fD0KE6AdeiReDnq9zWrbB9u10QNcaEl8JRQ9+7F6ZO1SHQXotW+JPafm4J3RgTTgpHDf2LL3S6xFtvDah4XJxOutiwYR7HZYwxuSjsEvqyZTo5YurPZ5/5KBQXp2P1N27U+xMm6EQsPob3e/vjD3j7bV1OtGVLvy0yxhQo7du3P2OQ0MiRIxk0aFCmjytTpgwAO3bsoEePHn6PvWTJkkyPM3LkSI4cOZJ2/9prr2X//v2BhG4y8tefMa9/ctoP/dVXtX+4909aV83du52bNMm50qV1R0yMczNnOifi3DPPZHnswYNPH/P113MUnjHZFup+6P/973/P6DPdokULN3fu3EwfV7p06SyP3a5dO7d48eJMy1xwwQVu9+7dWQeaT506dcqdPHkyT46d3X7oYVdDv3+QY/fG/ezerdc1AeLmnNAeLFWq6OIUNWvCe+/pykPXXKN9D/v3z/LYP/8M7drBnj05nnHUmLMSitlze/TowTfffMNff/0FwJYtW9ixYwdt27ZN6xfepEkTGjZsyNSpU894/JYtW4iKigJ0WP4tt9xCdHQ0vXr1ShtuD9o/O3Xq3WeffRaAN998kx07dnDllVdy5ZVXAjokPykpCYARI0YQFRVFVFRU2tS7W7Zs4bLLLuOee+6hQYMGdOrUKd3zpPr6669p0aIFjRs3pmPHjuzatQvQvu79+vWjYcOGREdHp00dMGPGDJo0aUJMTAxXXXUVoPPDv/baa2nHjIqKYsuWLWkxDBo0iCZNmrBt2zafrw9g8eLFtG7dmpiYGJo3b87Bgwe5/PLL000L3KZNG1auXJn5HyoAYdeoUGrs25R6/nkYP55KtS6gSqnziX9iBvcc/g8MHAjdu0Pr1jpHy+HDsHs3DBkCXpPt+HLgAKxaBf/8JwQwGZsxBUalSpVo3rw5M2bM4IYbbmDixIn06tULEaFEiRJMmTKFcuXKkZSURMuWLenWrZvf9S5Hjx5NqVKlWLlyJStXrqSJVzPnCy+8wDnnnMPJkye56qqrWLlyJQ8++CAjRoxgzpw5VK5cOd2xli5dyocffsjChQtxztGiRQvatWtHxYoV2bhxIxMmTODdd9/l5ptv5osvvuC2225L9/i2bduyYMECRIT33nuPV199leHDh/Pvf/+b8uXLs2rVKgD27dvH7t27ueeee5g7dy516tRJNy+LP+vXr+fDDz/k7bff9vv66tWrR69evZg0aRLNmjXjwIEDlCxZkv79+zN27FhGjhzJhg0b+Ouvv4iOjs7W382XsEvotG2rGbdTJwRow5fEl2ylCzt36pS+7IMPBnzYBQt07VDr2WJCKVSz5/bu3ZuJEyemJfTUOcydczz11FPMnTuXIkWKsH37dnbt2sV5553n8zhz587lQc//XXR0dLokNXnyZMaMGUNKSgo7d+5k7dq1mSaxuLg4brzxxrSZD7t3787PP/9Mt27dqFOnDo0aNQLST7/rLTExkV69erFz506OHz9OnTp1AJ1Od+LEiWnlKlasyNdff80VV1yRViaQKXYvuOACWnpNxerr9YkI1apVS5uCuJxn3qiePXvy73//m2HDhvHBBx/Qt2/fLJ8vEAE1uYhIZxFZLyKbRGSIj/3FRWSSZ/9CEamdK9H5EhMDixdr98PXXqPtPzuw6Wh1dsV0yvqxmYiL0zWhbapcUxj97W9/44cffmDZsmUcPXo0rWb9ySefsHv3bpYuXcqKFSs499xzfU6Z681X7f23337jtdde44cffmDlypVcd911WR7HZTKVburUu+B/it4HHniAwYMHs2rVKt55552053M+pr31tQ3ST7EL6afZ9Z5i19/r83fcUqVKcfXVVzN16lQmT57MrQH2wMtKlgldRCKAt4AuQH2gt4jUz1DsbmCfc64u8DrwSq5E50+pUjpA6NFHaXOtzsSW2nc8p+Lj9bOibNlciM+YMFOmTBnat2/PXXfdRe/evdO2p04dGxkZyZw5c/j9998zPc4VV1yRthD06tWr09qFDxw4QOnSpSlfvjy7du3iu+++S3tM2bJlOXjwoM9jffXVVxw5coTDhw8zZcoULs/G8O3k5GSqV68OwLhx49K2d+rUiVGjRqXd37dvH61ateJ///sfv/32G5B+it1ly5YBsGzZsrT9Gfl7ffXq1WPHjh0sXrwYgIMHD6Z9+PTv358HH3yQZs2aBfSNIBCBNLk0BzY5534FEJGJwA2A90TFNwDPeX7/HBglIuIy+4jNJU2aQIkS2nz+z3/m/DgbNsB99+VeXMaEm969e9O9e/d0zRF9+vTh+uuvT5t6NqvFGgYOHEi/fv2Ijo6mUaNGNG/eHNDVhxo3bkyDBg248MIL0029O2DAALp06UK1atWYM2dO2vYmTZrQt2/ftGP079+fxo0b+2xe8eW5556jZ8+eVK9enZYtW6Yl46effpr777+fqKgoIiIiePbZZ+nevTtjxoyhe/funDp1iqpVqzJr1ixuuukmxo8fT6NGjWjWrBmXXHKJz+fy9/qKFSvGpEmTeOCBBzh69CglS5Zk9uzZlClThqZNm1KuXLlcnTNdssq5ItID6Oyc6++5fzvQwjk32KvMak+ZRM/9zZ4ySRmONQAYAFCrVq2mWX3aB+qNN7TJ5GxERMDTT4PnYr0xQbNu3Touu+yyUIdhgmzHjh20b9+eX375hSJFfDeW+HpviMhS51ysr/KB1NB9Xc7O+CkQSBmcc2OAMQCxsbG5Vnt/6CH9McaYcDB+/HiGDh3KiBEj/CbznAgkoScCNb3u1wB2+CmTKCJFgfJA1v1+jDGmELrjjju44447cv24gXw0LAYuFpE6IlIMuAWYlqHMNOBOz+89gB+D0X5uTEFh/y4mo5y8J7JM6M65FGAwMBNYB0x2zq0RkX+JSDdPsfeBSiKyCfg7cEbXRmOMbyVKlGDPnj2W1E0a5xx79uyhRIkS2XpclhdF80psbKzLatIeYwqDEydOkJiYmGW/bFO4lChRgho1ahAZGZlu+9leFDXG5KHIyMi0EYrGnI2wm5zLGGOMb5bQjTGmgLCEbowxBUTILoqKyG4gp0NFKwNJWZYKjfwam8WVPRZX9uXX2ApaXBc456r42hGyhH42RGSJv6u8oZZfY7O4ssfiyr78GlthisuaXIwxpoCwhG6MMQVEuCb0MaEOIBP5NTaLK3ssruzLr7EVmrjCsg3dGGPMmcK1hm6MMSYDS+jGGFNAhF1Cz2rB6iDGUVNE5ojIOhFZIyIPebY/JyLbRWSF5+faEMS2RURWeZ5/iWfbOSIyS0Q2em4rBjmmS73OyQoROSAiD4fqfInIByLyp2e1rdRtPs+RqDc977mVItIkyHENE5FfPM89RUQqeLbXFpGjXufuv0GOy+/fTkT+4Tlf60XkmryKK5PYJnnFtUVEVni2B+WcZZIf8vY95pwLmx8gAtgMXAgUAxKA+iGKpRrQxPN7WWADuoj2c8BjIT5PW4DKGba9Cgzx/D4EeCXEf8c/gAtCdb6AK4AmwOqszhFwLfAdujJXS2BhkOPqBBT1/P6KV1y1vcuF4Hz5/Nt5/g8SgOJAHc//bEQwY8uwfzjwTDDPWSb5IU/fY+FWQ09bsNo5dxxIXbA66JxzO51zyzy/H0Tniq8eilgCdAOQuvT5OOBvIYzlKmCzcy53FpXNAefcXM5cVcvfOboBGO/UAqCCiFQLVlzOue+drksAsABdNSyo/Jwvf24AJjrn/nLO/QZsQv93gx6biAhwMzAhr57fT0z+8kOevsfCLaFXB7Z53U8kHyRREakNNAYWejYN9nxt+iDYTRseDvheRJaKLswNcK5zbifomw2oGoK4Ut1C+n+wUJ+vVP7OUX56392F1uRS1RGR5SLyPxG5PATx+Prb5afzdTmwyzm30WtbUM9ZhvyQp++xcEvoAS1GHUwiUgb4AnjYOXcAGA1cBDQCdqJf94KtjXOuCdAFuF9ErghBDD6JLmPYDfjMsyk/nK+s5Iv3nYgMBVKATzybdgK1nHON0ZXCPhWRckEMyd/fLl+cL4/epK88BPWc+cgPfov62JbtcxZuCT2QBauDRkQi0T/WJ865LwGcc7uccyedc6eAd8nDr5r+OOd2eG7/BKZ4YtiV+hXOc/tnsOPy6AIsc87t8sQY8vPlxd85Cvn7TkTuBLoCfZyn0dXTpLHH8/tStK36kmDFlMnfLuTnC0B0wfruwKTUbcE8Z77yA3n8Hgu3hB7IgtVB4Wmbex9Y55wb4bXdu93rRmB1xsfmcVylRaRs6u/oBbXVpF/I+05gajDj8pKuxhTq85WBv3M0DbjD0xOhJZCc+rU5GESkM/Ak0M05d8RrexURifD8fiFwMfBrEOPy97ebBtwiIsVFpI4nrkXBistLR+AX51xi6oZgnTN/+YG8fo/l9dXePLh6fC16xXgzMDSEcbRFvxKtBFZ4fq4FPgJWebZPA6oFOa4L0R4GCcCa1HMEVAJ+ADZ6bs8JwTkrBewBynttC8n5Qj9UdgIn0NrR3f7OEfp1+C3Pe24VEBvkuDah7aup77P/esre5PkbJwDLgOuDHJffvx0w1HO+1gNdgv239GwfC9yXoWxQzlkm+SFP32M29N8YYwqIcGtyMcYY44cldGOMKSAsoRtjTAFhCd0YYwoIS+jGGFNAWEI3xpgCwhK6McYUEP8PMeGyJliH3MoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epochs=100;\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
